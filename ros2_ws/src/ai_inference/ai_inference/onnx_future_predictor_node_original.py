#!/usr/bin/env python3
import rclpy
from rclpy.node import Node
import numpy as np
from std_msgs.msg import Float32MultiArray
import time # ì‹œê°„ ì¸¡ì •ìš©

from hailo_platform import (HEF, VDevice, HailoStreamInterface, InferVStreams, 
                            ConfigureParams, InputVStreamParams, OutputVStreamParams, FormatType)

# =========================
#  CONFIGURATION
# =========================
GRID_SIZE = 256
T_IN = 10   
HEF_PATH = "/home/ubuntu/ros2_ws/src/ai_inference/ai_inference/early_fusion.hef" 

class HailoFuturePredictor(Node):
    def __init__(self):
        super().__init__("hailo_future_predictor")
        
        self.pipeline = None
        self.input_name = None
        self.output_name = None

        self.get_logger().info(f"ðŸ“¦ Loading HEF model: {HEF_PATH}")
        
        try:
            self.target = VDevice()
            self.hef = HEF(HEF_PATH)

            self.configure_params = ConfigureParams.create_from_hef(
                self.hef, interface=HailoStreamInterface.PCIe
            )
            
            self.network_groups = self.target.configure(self.hef, self.configure_params)
            self.network_group = self.network_groups[0]
            self.network_group_params = self.network_group.create_params()

            self.input_params = InputVStreamParams.make(
                self.network_group, format_type=FormatType.FLOAT32
            )
            self.output_params = OutputVStreamParams.make(
                self.network_group, format_type=FormatType.FLOAT32
            )

            self.input_vstream_infos = self.hef.get_input_vstream_infos()
            self.output_vstream_infos = self.hef.get_output_vstream_infos()
            
            self.input_name = self.input_vstream_infos[0].name
            self.output_name = self.output_vstream_infos[0].name
            
            self.get_logger().info(f"âœ” Model Loaded. Input: {self.input_name}, Output: {self.output_name}")

        except Exception as e:
            self.get_logger().error(f"âŒ Init Failed: {e}")
            raise e

        self.sub = self.create_subscription(
            Float32MultiArray,
            "/bev_pose_sequence",
            self.on_bev_sequence,
            10
        )
        self.pub = self.create_publisher(
            Float32MultiArray,
            "/future_occupancy",
            10
        )
        
        self.get_logger().info("ðŸ”® Node Initialized. Waiting for data...")

    def set_pipeline(self, pipeline):
        self.pipeline = pipeline

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def on_bev_sequence(self, msg: Float32MultiArray):
        if self.pipeline is None:
            self.get_logger().warn("Pipeline not ready.")
            return

        # [DEBUG 1] ë°ì´í„° ìˆ˜ì‹  í™•ì¸
        # self.get_logger().info(f"ðŸ“¥ Received Data! Size: {len(msg.data)}")
        
        seq_flat = np.array(msg.data, dtype=np.float32)
        
        try:
            bev_nchw = seq_flat.reshape(T_IN, GRID_SIZE, GRID_SIZE)
            
            # [DEBUG 2] ì „ì²˜ë¦¬ ì‹œìž‘
            bev_nhwc = np.ascontiguousarray(bev_nchw.transpose(1, 2, 0))
            input_tensor = bev_nhwc[np.newaxis, ...].astype(np.float32)

            # [DEBUG 3] ì¶”ë¡  ì§ì „ (ì—¬ê¸°ì„œ ë©ˆì¶”ë©´ Hailo ìž¥ì¹˜ ë¬¸ì œ)
            # self.get_logger().info("ðŸš€ Inferencing...")
            start_time = time.time()

            input_data = {self.input_name: input_tensor}
            
            # === ì—¬ê¸°ê°€ í•µì‹¬ ë³‘ëª© êµ¬ê°„ ===
            infer_results = self.pipeline.infer(input_data) 
            # ==========================

            end_time = time.time()
            # [DEBUG 4] ì¶”ë¡  ì™„ë£Œ (ì—¬ê¸°ê¹Œì§€ ì˜¤ë©´ ì„±ê³µ)
            # self.get_logger().info(f"âœ… Done! Time: {(end_time - start_time):.4f}s")
            
            raw_output = infer_results[self.output_name]
            future_prob = self.sigmoid(raw_output)

            future_prob_nchw = future_prob.transpose(0, 3, 1, 2)
            
            out_msg = Float32MultiArray()
            out_msg.data = future_prob_nchw.flatten().tolist()
            self.pub.publish(out_msg)

        except Exception as e:
            self.get_logger().error(f"âŒ Processing Error: {e}")

def main(args=None):
    rclpy.init(args=args)
    node = HailoFuturePredictor()

    try:
        # [ìˆ˜ì •] self.get_logger() -> node.get_logger() ë¡œ ë³€ê²½
        node.get_logger().info("ðŸ”Œ Activating Network Group...")
        
        with node.network_group.activate(node.network_group_params):
            node.get_logger().info("ðŸŒŠ Opening VStreams...") # [ìˆ˜ì •]
            
            with InferVStreams(node.network_group, node.input_params, node.output_params) as pipeline:
                
                node.set_pipeline(pipeline)
                node.get_logger().info("âœ… ALL READY! Spin Start.") # [ìˆ˜ì •]
                
                rclpy.spin(node)

    except KeyboardInterrupt:
        pass
    except Exception as e:
        node.get_logger().error(f"Critical Error: {e}") # [ìˆ˜ì •]
    finally:
        node.destroy_node()
        if rclpy.ok():
            rclpy.shutdown()

if __name__ == "__main__":
    main()